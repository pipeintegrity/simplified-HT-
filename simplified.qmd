---
title: "Simplified Hyperbolic Tangent"
author: "Authors TBD"
format: docx
editor: visual3
chunk_output_type: console
execute:
  echo: false
  warning: false
  fig-width: 6.5
---

# Hyperbolic Tangent

When performing Charpy V-notch testing over multiple temperatures the result will be a sigmoidal-shaped curve that is asymptotic at the upper and lower temperatures as seen in @fig-pltdata. But the relationship between the temperature and Absorbed Energy (AE) or Shear Area (SA) is not the classic sigmoid function but a hyperbolic tangent (HT) function with four independent coefficients (A, B, C, D) shown in @eq-HT.

```{r}
#| label: read-data

library(readxl)
library(tidyverse)
library(tidymodels)
library(here)
library(minpack.lm)
library(patchwork)
library(latex2exp)

theme_set(theme_minimal(13))
pal <- RColorBrewer::brewer.pal(3,"Set1")

## Read data in------------------------------------
tim_cvn <-
  read_excel(here("timp_met_charpy_clean_rev02.xlsx"),
             sheet = "Charpy") %>%
  janitor::clean_names() %>%
  rename(
    location = specimen_location,
    temp_f = test_temperature_f,
    charpy_thickness_mm = specimen_thickness_mm,
    cvn_ss_ft_lbs = measured_energy_absorbed_ft_lbs,
    sa_ss = shear_area_percent
  ) %>%
  mutate(across(temp_f:sa_ss, as.numeric),
         sa_ss = as.numeric(str_remove(
           string = sa_ss, pattern = c("<", ">")
         ))) %>%
  filter(
    str_detect(location, "Base Metal"),
    #removing weld tests
    str_detect(single_or_multiple_temperature_test, "Multiple"),
    #remove single temp tests
    str_detect(full_transition_curve, "Yes"),
    sa_ss <= 100
  ) %>% #removing shear areas greater than 100
  select(id,
         bar_orientation,
         charpy_thickness_mm,
         temp_f,
         cvn_ss_ft_lbs,
         sa_ss) %>% 
  rename(cvn = cvn_ss_ft_lbs)
 
#list of id that max is less than 90%
idlist <- tim_cvn %>% 
  group_by(id) %>% 
  summarise(mx_sa=max(sa_ss)) %>% 
  filter(mx_sa<90)

temp_id_list <- tim_cvn %>% 
  group_by(id, temp_f) %>% 
  summarise(unique(temp_f)) %>% 
  mutate(count = n()) %>% 
  filter(count<4)

`%notin%` <- Negate(`%in%`) # not in special function

tim_cvn <- tim_cvn %>% 
  filter(id %notin% idlist$id,
         id %notin% temp_id_list$id)

ids <- unique(tim_cvn$id) #create vector of ids

## Select which id to use ----------------------------------------
idx <- 3 #start index at 1

# inital estiamtes for A, B, C, D for SA solution

cvn_data <- tim_cvn %>% 
  filter(id==ids[idx])
```

```{r}
#| label: fit-data

Ai_sa <- 51 #initial estimates A + B <= 100
Bi_sa <- 49 #initial estimates
Ci <-  IQR(cvn_data$temp_f) / 2 #inner quartile range/2 = Ci
Di <- (cvn_data$temp_f[which.min(abs(50 - cvn_data$sa_ss))]) 
# Di = what temp is closest to 50% SA


# hyperbolic tangent function
func <- function(t, A, B, C, D) {
  A + B * tanh((t - D) / C)
}


# SA solution ----------------------------------------------
fit_sa <-
  nlsLM(
    sa_ss ~ func(temp_f, A, B, C, D),
    data = cvn_data ,
    start = list(
      A = Ai_sa,
      B = Bi_sa,
      C = Ci,
      D = Di
    ),
    trace = F
  )

# simplified SA HT method ------------
fit_sa_simp <-
  nlsLM(
    sa_ss ~ func(temp_f,Ai_sa, Bi_sa, C, D),
    data = cvn_data ,
    start = list(
      C = Ci,
      D = Di
    ),
    trace = F
  )


SA_coefs_gen <- tidy(fit_sa,conf.int = TRUE) %>% 
  mutate(model="General Solution")

SA_coefs_simp <- tidy(fit_sa_simp,conf.int = TRUE) %>% 
  mutate(model="Simplified HT")

# linearize the data -----------------------------------------
cvn_data <- cvn_data %>%
  mutate(k1 = atanh((sa_ss - Ai_sa) / Bi_sa))

# AE solution -------------------------------------------------

# intital values for AE solution
ae_init <- cvn_data %>% 
  summarise(Ai_ae = max(cvn)/2,
                     Bi_ae = (max(cvn) - min(cvn))/2)


fit_cvn <-
  nlsLM(
    cvn ~ func(temp_f, A, B, C, D),
    data = cvn_data ,
    start = list(
      A = ae_init$Ai_ae,
      B = ae_init$Bi_ae,
      C = Ci,
      D = Di
    ),
    trace = F
  )


sa_sols <- tibble(C = SA_coefs_simp$estimate[1], D =SA_coefs_simp$estimate[2])

fit_cvn_simp <-
  nlsLM(
    cvn ~ func(temp_f, A, B, sa_sols$C, sa_sols$D),
    data = cvn_data ,
    start = list(
      A = ae_init$Ai_ae,
      B = ae_init$Bi_ae
    ),
    trace = F
  )

AE_gen_coefs <- tidy(fit_cvn, conf.int = TRUE) %>% 
  mutate(model = "General Solution")

AE_coefs_simp <- tidy(fit_cvn_simp, conf.int = TRUE) %>% 
  mutate(model = "Simplified HT")

```

```{r}
#| label: fig-pltdata
#| fig-cap: "Example CVN Data"
#| dpi: 300
#| fig-height: 5

# Note: put a space between "#|" and parameter otherwise it won't recognize it

sa_plt <- cvn_data %>% 
  ggplot(aes(temp_f, sa_ss))+
  geom_point()+
  labs(title = "Shear Area vs. Temperature",
       x = sprintf("Temperature (\u00B0F)"), 
       y = "Shear Area (%)")

cvn_plt <- cvn_data %>% 
  ggplot(aes(temp_f, cvn))+
  geom_point()+
  labs(title = "Absorbed Energy vs. Temperature",
       x = sprintf("Temperature (\u00B0F)"),
       y = "Absorbed Energy (ft-lbs)")

sa_plt /cvn_plt

```

$$
\begin{align}
&SA \ (or\ AE)=A +B \ tanh\left(\frac{T- D}{C}\right)\\
&Where:\\
&A,B,C,D = Regression \ Coefficients\\
&T = Temperature\\&SA = Shear \ Area\\ 
&AE = Absorbed \ Energy 
\end{align}
$$ {#eq-HT}

The physical meaning of the four coefficients is shown in @fig-HT. The upper shelf absorbed energy or maximum shear area are equal to $A + B$ and the lower shelf and minimum shear area are $A - B$. The shear appearance transition temperature (SATT) is the temperature that corresponds to 85% shear area. If the material is tested throughout the entire temperature range so as to capture all or part of the asymptotic portion of the curves the upper and lower ranges can be approximated by plotting the data. However, if only part of the curve is tested, the upper and lower shelf is a rough guess at best and estimating the SATT is even more difficult without regressing the data to the hyperbolic tangent.

```{r}
#| label: fig-HT
#| fig-cap: "Hyperbolic Tangent"
#| dpi: 300

tdy_cvn <- tidy(fit_cvn)
A <- tdy_cvn$estimate[1]
B <- tdy_cvn$estimate[2]
C <- tdy_cvn$estimate[3]
D <- tdy_cvn$estimate[4]

US = A + B
LS = A - B

mtf <- min(cvn_data$temp_f)

T2 <- cvn_data %>% slice_max(order_by = cvn, n = 2) #top 2
B2 <- cvn_data %>% slice_min(order_by = cvn, n = 2) #bottom 2
slope <- B / C # slope of tangent

AL <- mean(T2$temp_f-(D+C))#Arc length
theta <- atan(slope) #Angle
d <- AL *sin(theta) #vertical distance of arc
l_prime <- AL*cos(theta) #Hztl. projection of arc
mint <-mtf
maxt <- max(cvn_data$temp_f)
# Calculate locations for labels -----------------------------------

AD <- tibble(
  x = c(mtf,
        D,
        D),
  y = c(A,
        A,
        0)
)

OD <- tibble(
  x = c(
    mtf,
    D - C,
    D + 1.5 * C
  ),
  y = c(
    LS,
    LS,
    LS +
      2.5 * C * slope
  )
)

LU <-
  tibble(
    x = c(
     D + C,
      D + C
    ),
    y = c(LS-6, 
         US)
  )

CV = tibble(x = c(mean(T2$temp_f), mean(T2$temp_f)-7.5),
            y = c(US, US + slope*(mean(T2$temp_f)-7.5 - D - C)))

minc <- min(cvn_data$cvn)
##  Plot function -----------------------------------------------
plt_cvn <- cvn_data %>%
  ggplot(aes(temp_f, cvn)) +
  geom_point() +
  stat_function(fun = func,
                args = list(A,
                            B,
                            C,
                            D),
                col='orangered') +
  theme_minimal() +
  labs(
       x ="Temperature", 
       y = "Absorbed Energy or Shear Area"
       )+
  theme(axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(),
        plot.margin = margin(0.5,0.5,0.5,0.5,"cm"))

# annotate plot ---------------------------------------
plt_cvn +
  geom_line(data = AD,
            aes(x, y),
            lty = 2,
            col = 'grey25', alpha=0.75) +
  geom_line(data = OD,
            aes(x, y),
            lty = 1,
            col = 'grey25', alpha=0.75) +
  geom_line(data = LU,
            aes(x, y),
            lty = 1,
            col = 'grey25', alpha=0.75) +
  geom_linerange(aes(x = D - C, ymin = 0, ymax = LS),
                 col='grey25', alpha=0.75) +
  geom_linerange(aes(x = D + C, ymin = 0, ymax = US), 
                 col='grey25', alpha=0.75) +
  geom_segment(
    data = NULL,
    aes(
      x = D - C,
      xend = D,
      y = minc - 1,
      yend = minc - 1
    ) ,
    arrow = arrow(
      ends = "both",
      length = unit(0.075, "inches"),
      type = "closed"
    )
  ) +
  geom_segment(
    data = NULL,
    aes(
      x = D ,
      xend = D + C,
      y = minc - 1,
      yend = minc - 1
    ) ,
    arrow = arrow(
      ends = "both",
      length = unit(0.075, "inches"),
      type = "closed"
    )
  ) +
  geom_hline(yintercept = A + B,
             col = 'grey25', alpha=0.75) +
  geom_curve(
    data = CV,
    aes(
      x = x[1],
      xend = x[2],
      y = y[1],
      yend = y[2]
    ),
    arrow = arrow(
      ends = "both",
      length = unit(0.075, "inches"),
      type = "closed"
    )
  ) +
  annotate(
    "text",
    x = mean(T2$temp_f) + 7.5,
    y = sum(CV$y) / 2 + 2,
    label = "B/C"
  ) +
  annotate(
    "text",
    x = c(sum(D, D - C) / 2, sum(D, D + C) / 2),
    y = c(minc + 2, minc + 2),
    label = c("C", "C")
  ) +
  annotate("text",
           x = D,
           y = LS - 5,
           label = "D") +
  geom_segment(
    aes(
      x = mint + 5,
      xend = mint + 5,
      y = LS,
      yend = A
    ),
    arrow = arrow(
      ends = "both",
      length = unit(0.075, "inches"),
      type = "closed"
    )
  ) +
  geom_segment(
    aes(
      x = mint + 5,
      xend = mint + 5,
      y = US,
      yend = A
    ),
    arrow = arrow(
      ends = "both",
      length = unit(0.075, "inches"),
      type = "closed"
    )
  ) +
  annotate(
    "text",
    x = c(mint + 10, mint + 10),
    y = c(mean(c(LS, A)), mean(c(US, A))),
    label = c("B", "B")
  ) +
  annotate("text",
           x = mint - 5,
           y = A,
           label = "A") 
```

# Solution to Hyperbolic Tangent

Unfortunately, there is no closed-form solution to @eq-HT, solving it requires one of several numerical methods known as non-linear least squares (NLS) regression. NLS attempts to fit the observed data to a form of $y = f(x, \beta)$ where f is a some nonlinear function such as an exponential or trigonometric functions through successive approximations. Any type of regression technique (non-linear or linear) seeks to minimize the sum of the squares of the residuals (RSS). The residual is the difference between the observed (measured) $y$ value, $y_i$ and the predicted value of the model $\hat{y}$.

Therefore the quantity to minimize is shown in @eq-rss.

$$
RSS = \sum_{i =1}^n (y_i - \hat{y})^2
$$ {#eq-rss}

Since the ability to solve this non-linear regression is relying on numerical optimization methods it requires initial starting values for the four coefficients for the algorithm. The down side to some of these methods is that if the initial values are poor or if there are several outliers in the data the solution can fail to converge or get "stuck" in a local minimum and return a solution that is far from optimal. Although most algorithms have methodologies to avoid getting stuck in a local minimum it is quite common for the NLS to fail with poor initial values especially when there are outliers in the data as well.

However, with some of simplifying assumptions the hyperbolic non-linear tangent can be transformed to a linear regression model that does have a closed-form solution to minimize the RSS through ordinary least squares regression (OLS) regression of the form $y_i = \alpha x_i + \beta + \epsilon_i$ where $\epsilon_i$ is the residual error. The benefit to the OLS regression methodology is that since the solution is closed-form it is guaranteed to find the optimum solution for the given data, in addition it can be done in a spreadsheet or even by hand if needed. In the OLS solution, $\alpha$ and $\beta$ are estimated such as to minimize $\epsilon_i$. Since it is a given that the minimum and maximum percent SA is physically bounded by 0 and 100 therefore $A + B\leq 100$ and $A - B \leq 100$. Also looking at @fig-HT it can be seen that the value of $A$ and $B$ are similar with $A$ being only slightly larger than $B$. Therefore a reasonable initial estimate for $A$ and $B$ for the SA solution would be about 51 and 49 respectively. Now that $A$ and $B$ are fixed values, that leaves the @eq-HT with only two unknowns and rearranging the terms and solving for temperature produces the more useful form of $T = C \ atanh\left(\frac{SA - A}{B}\right)+ D$ and since everything in the argument for the $atanh$ is known, the entire $atanh$ term becomes an independent variable $k_1 = atanh\left(\frac{SA -A}{B}\right)$ and the HT function is transformed into a linear equation, $T = C\ k_1+D$ which can be solved by OLS regression. The solution to this will produce two coefficients, slope and intercept corresponding to the $C$ and $D$ respectively. Referring to @fig-HT observe that $D$ is the center of the linear transition area between the upper and lower asymptote and and appears to be approximately the same for SA or AE when looking at @fig-pltdata. Using this assumption and the solution from SA, @eq-HT can again be linearized to be solved by OLS regression. Since $C_{sa}$ and $D_{sa}$ (subscript indicates that they are from the SA solution) are known from the SA solution, inserting them into @eq-HT, all the terms in the $tanh$ function are known and it can be reduced to $k_2 = tanh\left(\frac{T-D_{sa}}{C_{sa}}\right)$ and the HT equation becomes, $AE = A + B \ k_2$ which can also be solved with OLS regression.

Using the same simplifying assumptions mentioned in this section, the problem can also be solved using NLS for the two remaining coefficients. This can be helpful if the NLS solution does not converge when trying to solve all four coefficients at once for some reason. Though the linearized form of the HT is functionally equivalent to the two term hyperbolic tangent the OLS solution can be influenced by data points that are far from the mean and have high residuals, what are known as high *leverage* points. These single points can have disproportionate influence on the slope of the OLS regression as where they have far less influence in a NLS regression solution since that is the region that are asymptotic to the minimum or maximum values. In these cases with high leverage points are present, either removing the high residual observation if they seem to be erroneous or using the two term NLS solution may be preferable. In @lo_data the solutions for absorbed energy and shear area are shown with a set of data that has significant scatter.

# Results

While this appears to be a workable solution, it does involve some assumptions. To judge the suitability of this solution it will be used to solve the example data shown in @fig-pltdata and the results will be compared to the general solution where all four coefficients are solved simultaneously without the simplifying assumptions. Since any model is an approximation to a physical phenomena and any measurement is going to have some amount of error regardless of precision there is uncertainty in the solutions which are represented by the standard errors for the coefficients. Note that since $A$ and $B$ were assumed for the SA solution there is no confidence intervals or standard error for the OLS solution so they are set to zero. The results are plotted in @fig-sa.coefsCI.

```{r}
#| label: tbl-results
#| tbl-cap: "Comparison of Results"

fit_sa_lm <- lm(temp_f ~ k1, data = cvn_data) 

sa_lm_coefs <- tidy(fit_sa_lm, conf.int = TRUE) %>%
  mutate(term = ifelse(term == "k1", "C", "D"))

sa_coefs_all <-  sa_lm_coefs %>%
  bind_rows(tibble(term = c("A", "B"),
                   estimate = c(51, 49))) %>%
  mutate(model = "Simplified OLS") %>%
  bind_rows(SA_coefs_simp) %>% 
  bind_rows(tibble(term = c("A", "B"),
                   estimate = c(51, 49), model = "Simplified HT")) %>%
  mutate(term = case_when(term == "(Intercept)" ~ "D",
                          term == "k1" ~ "C",
                          TRUE ~ term)) %>%
  bind_rows(SA_coefs_gen) %>%
  mutate(across(.cols = everything(), ~ ifelse(is.na(.), 0, .))) %>%
  arrange(term, model)

sa_coefs_all %>%
  select(-c(p.value, statistic)) %>%
  mutate(across(.cols = where(is.numeric), ~ round(., 2))) %>%
  kableExtra::kable(format = "pipe")


```

# Shear Area Coefficients

```{r}
#| label: fig-sa.coefsCI
#| fig-cap: "Coefficient Confidence Intervals for Shear Area Solutions"
#| fig-width: 6.5

sa_coefs_all %>%
  filter(term == "C" | term == "D") %>%
  ggplot() +
  geom_errorbarh(aes(
    xmin = conf.low,
    xmax = conf.high,
    y = model,
    col = model
  ),
  lwd = 1) +
  geom_point(aes(estimate, model), size = 3) +
  facet_wrap(~ term, scales = "free_x") +
  labs(x = "Termperature (\u00B0F)",
       y = "Method",
       col = "Method") +
  scale_color_brewer(palette = "Set1")

```

# Plot of SA Solutions

Though there are visible differences in the model fit that can be seen in @fig-solplt especially through the transition area. However, at the 85% shear appearance area the two models start to converge though some differences are still evident. To demonstrate these differences the 95% confidence intervals are shown in the plot.

```{r}
#| label: fig-solplt
#| fig-cap: "Solutions Plot"
#| dpi: 300
#| fig-width: 6.5

func_ht <- function(temp_f, A, B, C, D) {
  A + B * tanh((temp_f - D) / C)
}

cols <- c("General" = pal[1], "Simplified OLS" = pal[3], "Simplified HT" = pal[2])

cvn_data %>%
  ggplot(aes(temp_f, sa_ss)) +
  geom_point() +
  stat_function(
    fun = func_ht,
    args = list(
      sa_coefs_all$estimate[1],
      sa_coefs_all$estimate[4],
      sa_coefs_all$estimate[7],
      sa_coefs_all$estimate[10]
    ),
    aes(col = "General"),
    lwd = 1, 
    n = 301
  ) +
   stat_function(
    fun = func_ht,
    args = list(
      sa_coefs_all$estimate[2],
      sa_coefs_all$estimate[5],
      sa_coefs_all$estimate[8],
      sa_coefs_all$estimate[11]
    ),
    aes(col = "Simplified HT"),
    lwd =1, 
    n = 301
  ) +
   stat_function(
    fun = func_ht,
    args = list(
      sa_coefs_all$estimate[3],
      sa_coefs_all$estimate[6],
      sa_coefs_all$estimate[9],
      sa_coefs_all$estimate[12]
    ),
    aes(col = "Simplified OLS"),
    lwd =1, 
    n = 301
  ) +
  geom_hline(yintercept = 85, 
             col='grey50', 
             lty =2, 
             lwd = 0.65)+
  scale_y_continuous(breaks = scales::pretty_breaks())+
  labs(x = "Temperature (\u00B0F)", 
       y = "Shear Area (%)")+
  annotate("text", x = 20, y = 85, label = "SATT", vjust = -0.5)+
  scale_color_manual(values = cols,
                     breaks = c("General", "Simplified HT","Simplified OLS" ), 
                     name = "Solution")

```

# Comparison of SATT

This compares the confidence intervals of the SATT for the two methods . The mean estimated SATT is nearly equal for the two methods but the width of the confidence interval for the OLS method is smaller due to $A$ and $B$ being fixed with no error associated with them. Since the two 95% confidence intervals overlap it indicates that given the uncertainty in either model the differences in the predicted SATT are not significant at a 95% confidence level.

```{r}
#| label: fig-satterr
#| fig-cap: "Confidence Interval of SATT"
#| fig-width: 6.5
#| dpi: 300


sa_wide = sa_coefs_all %>%
  pivot_wider(
    id_cols = model,
    names_from = term,
    values_from = c(estimate, conf.high, conf.low)
  )


satt_lm_CI <-  tidy(fit_sa_lm, conf.int = TRUE) %>%
 # group_by(model) %>%
  summarise(
    estimate = atanh((85 - Ai_sa) / Bi_sa) * estimate[2] + estimate[1],
    UCI = atanh((85 - Ai_sa) / Bi_sa) * conf.high[2] + conf.high[1],
    LCI = atanh((85 - Ai_sa) / Bi_sa) * conf.low[2] + conf.low[1]
  ) %>% 
  mutate(model = "Simplified OLS")


satt_simp_CI <-  tidy(fit_sa_simp, conf.int = TRUE) %>%
 # group_by(model) %>%
  summarise(
    estimate = atanh((85 - Ai_sa) / Bi_sa) * estimate[2] + estimate[1],
    UCI = atanh((85 - Ai_sa) / Bi_sa) * conf.high[2] + conf.high[1],
    LCI = atanh((85 - Ai_sa) / Bi_sa) * conf.low[2] + conf.low[1]
  ) %>% 
  mutate(model = "Simplified HT")


satt_gen_CI <-  SA_coefs_gen %>%
  # group_by(model) %>%
  summarise(
    estimate = atanh((85 - estimate[1]) / estimate[2]) * estimate[3] + estimate[4],
    UCI = atanh((85 - conf.low[1]) /conf.high[2]) * conf.high[3] + conf.high[4],
    LCI = atanh((85 - conf.high[1]) /conf.low[2]) * conf.low[3] + conf.low[4]
  ) %>% 
  mutate(model = "General Solution")

sa_cI_all <- bind_rows(satt_gen_CI, satt_simp_CI, satt_lm_CI)

sa_cI_all %>% 
  ggplot() +
  geom_errorbarh(aes(
    xmin = LCI,
    xmax = UCI,
    y = model,
    col = model
  ), lwd = 1) +
  geom_point(aes(estimate, model), size = 3)+
  labs(
    y = "Model",
    x = "Temperature (\u00B0F)",
    col = "Model"
  ) +
  scale_color_brewer(palette = "Set1")
  


```

# Comparison of Absorbed Energy

The next comparison is with the OLS method and the general solution for AE. First is the coefficients for both models. Note that the $C$ and $D$ coefficients are carried over from the SA solution.

```{r}
#| label: tbl-aecoefs
#| tbl-cap: "Absorbed Energy Coefficients" 

cvn_data <- cvn_data %>% 
  mutate(k2 = tanh((temp_f - sa_lm_coefs$estimate[1]) / sa_lm_coefs$estimate[2]))

ae_lm_mod <- lm(cvn ~ k2, data = cvn_data)

ae_lm_coefs <- tidy(ae_lm_mod, conf.int = TRUE) %>% 
  mutate(term = ifelse(term=="k2", "B", "A")) %>% 
  bind_rows(sa_lm_coefs) %>% 
  arrange(term) %>% 
  mutate(model = "Simplified OLS")

ae_simp_coefs <- tidy(fit_cvn_simp, conf.int = TRUE) %>% 
  bind_rows(SA_coefs_simp)%>% 
  arrange(term) %>% 
  mutate(model = "Simplified HT")


ae_coefs_all <- AE_gen_coefs %>% 
  bind_rows(ae_lm_coefs, ae_simp_coefs)

ae_coefs_all %>% 
   select(-c(p.value, statistic)) %>% 
  mutate(across(where(is.numeric), ~round(.,2))) %>% 
  arrange(term, model) %>% 
  kableExtra::kable(format = "pipe")

```

# Plot of Absorbed Energy Solution

This plot will show the HT solutions for the different types of solutions for comparison. With consistent data, the three solution methods are all approximately the same. See @fig-ae-error for a comparison of the uncertainty in the upper shelf estimates.

```{r}
#| label: fig-aeplot
#| fig-cap: "Plot of AE Solutions"
#| dpi: 300
#| fig-width: 6.5

ae_wide <- ae_coefs_all %>% 
  select(term, estimate, model) %>% 
  pivot_wider(id_cols = model,names_from = term, values_from = estimate)
  

cvn_data %>%
  ggplot(aes(temp_f, cvn)) +
  geom_point() +
  stat_function(
    fun = func_ht,
    args = list(ae_wide$A[1],
                ae_wide$B[1],
                ae_wide$C[1],
                ae_wide$D[1]),
    aes(col = "General"),
    lwd = 1,
    n = 301
  ) +
  stat_function(
    fun = func_ht,
    args = list(ae_wide$A[2],
                ae_wide$B[2],
                ae_wide$C[2],
                ae_wide$D[2]),
    aes(col = "Simplified HT"),
    lwd = 1,
    n = 301
  ) +
   stat_function(
    fun = func_ht,
    args = list(ae_wide$A[3],
                ae_wide$B[3],
                ae_wide$C[3],
                ae_wide$D[3]),
    aes(col = "Simplified OLS"),
    lwd = 1,
    n = 301
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(x = "Temperature (\u00B0F)",
       y = "Shear Area (%)") +
  scale_color_manual(
    values = cols,
    breaks = c("General", "Simplified HT", "Simplified OLS"),
    name = "Model"
  )


```

# Bootstraps

Because I can and bootstraps are awesome, end of story. ;)

When OLS regression is performed, the built-in assumption is that the error term, $\epsilon_i$ is normally distributed about the estimated mean. While this is a reasonable assumption most of the time, it can lead to biased estimates in some cases. In addition, OLS can be heavily influenced by outliers and a single regression solution can't inform the analyst if the solution would significantly change with or without the outliers. The solution to this problem is a technique known as bootstrapping. The way a bootstrap works is that if the original data set has $n$ observations then a sample of size $n$ is taken from the data set by randomly sampling one at a time with replacement. So so values are sampled more than once and some might not be at all. This process is repeated a large number of times, each time taking a new random sample with replacement. Then, a model is fit on each sample to see how the results change with different samples. The average of the model coefficients gives an unbiased estimate of the parameter with not underlying assumption of the distribution of it. In addition, model performance statistics can be collected for each model and aggregated to get a more complete picture of the uncertainty in things like the $R^2$. In @fig-aeboot 100 of the 1,000 bootstrapped models is plotted.

```{r}
#| label: fig-aeboot
#| fig-cap: "AE Bootstraps"
#| dpi: 300
#| fig-height: 5
#| fig-width: 6

nsim <- 1e3
# AE Bootstrap ------------------------------------------------------------

fit_sa_on_bootstrap2 <- function(split) {
  lm(
    ae_ss_J ~ k1,
    data = analysis(split)
  )
}

fit_ae_on_bootstrap2 <- function(split) {
  lm(
    cvn ~ k2,
    data = analysis(split)
  )
}

#Create Bootstrap data frame
boots <- cvn_data %>%
  bootstraps(times = nsim,
             apparent = FALSE)

# Create nested models
ae_models <-
  boots %>%
  mutate(
    model = map(splits, fit_ae_on_bootstrap2),
    coef_info = map(model, tidy),
    glanced = map(model, glance)
  )
# Bootstrap coefficients
ae_boot_coefs <-
  ae_models %>%
  unnest(coef_info)

# Bootstrap Performance
ae_boot_performance <-
  ae_models %>%
  unnest(glanced)

ae_params <- ae_boot_coefs %>%
  pivot_wider(id_cols = id,
              names_from = term,
              values_from = estimate) %>%
  rename(intercept = "(Intercept)") %>%
  mutate(id = 1:n())

#Bootstrap confidence intervals

bootCI <- int_pctl(ae_models, coef_info) %>% 
  mutate(model = "LM Bootstrap",
         term = ifelse(term=="k2","B","A")) %>% 
  rename(estimate =.estimate, conf.low =.lower,conf.high = .upper)
  
# Grab random sample of 100 solutions
ae_param_samp <- slice_sample(ae_params,n = 75)

# Regression samples plot -------------------------------------------------


ggplot(cvn_data,
       aes(k2, cvn)) +
  geom_point() +
  # geom_line(aes(y = predict(ae_lm_mod)),
  #           col = 'red',
  #           lwd = 0.25) +
  geom_abline(
    aes(
      slope = k2,
      intercept = intercept,
      group = id
    ),
    data = ae_param_samp,
    col = 'blue',
    alpha = 0.05
  ) +
  # geom_smooth(method = "lm", col='red')+
  labs(
    title = "Absorbed Energy",
    subtitle = "Bootsrap Resampling",
    # use latex in x title
    x = unname(TeX(
      "$k_2 = tanh \\left( \\frac{T -D_{SA}}{C_{SA}} \\right)$"
    )),
    y = "Absorbed Energy (ft-lbs)",
    caption = "100 Random Bootstraps"
  )

```

# Absorbed Energy Upper Shelf Confidence Intervals

This section will show the range of confidence intervals for upper shelf energy for the different methods. Since the confidence intervals overlap, it would not be rejected that the two solutions are equivalent at a 95% confidence level. The mean estimate for each of the methods is almost identical. The two simplified methods show a smaller range of uncertainty relative to the general. This is an artifact that two of the four parameters are fixed and therefore fewer ways for the model to reflect the uncertainty in the data.

```{r}
#| label: fig-ae-error
#| fig-cap: "Comparision of Upper Shelf Energy"
#| dpi: 300

ae_coefs_all %>% 
  select(estimate, conf.low, conf.high, term, model) %>%
  pivot_wider(
    id_cols = model,
    names_from = term,
    values_from = c(estimate, conf.low, conf.high)
  ) %>%
  group_by(model) %>%
  summarise(
    estimate = estimate_A + estimate_B,
    us_low = conf.low_A + conf.low_B,
    us_high = conf.high_A + conf.high_A
  ) %>%
  ggplot() +
  geom_errorbarh(aes(
    xmin = us_low,
    xmax = us_high,
    y = model,
    col = model
  ),
  lwd = 1)+
  geom_point(aes(estimate, model), size = 3)+
  labs(x = "Temperature (\u00B0F)",
       y = "Model",
       col = "Model")+
  scale_color_brewer(palette = "Set1")


```

# Less Optimal data {#lo_data}

In this section a set of data that exhibits significant scatter and inconsistencies is shown against the three types of solutions for both shear area and absorbed energy. In @fig-badsa-data it can be seen that the SATT is for either of the simplified solutions is approximately within$\pm$ of the general solution. Which is probably not a difference of any practical implications.

```{r}
#| label: fig-badsa-data
#| fig-cap: "Shear Area Solutions with High Scatter Data"
#| dpi: 300

## Select which id to use ----------------------------------------
idx <- 2 #start index at 1

# inital estiamtes for A, B, C, D for SA solution
Ai_sa <- 51 #initial estimates A + B <= 100
Bi_sa <- 49 #initial estimates

cvn_data_bad <- tim_cvn %>% 
  filter(id==ids[idx]) %>% 
  mutate(k1 = atanh((sa_ss - Ai_sa) / Bi_sa))

Ci <-  IQR(cvn_data_bad$temp_f) / 2 #inner quartile range/2 = Ci
Di <- (cvn_data_bad$temp_f[which.min(abs(50 - cvn_data_bad$sa_ss))]) 
# Di = what temp is closest to 50% SA

# SA solution ----------------------------------------------
fit_sa_bad <-
  nlsLM(
    sa_ss ~ func(temp_f, A, B, C, D),
    data = cvn_data_bad ,
    start = list(
      A = Ai_sa,
      B = Bi_sa,
      C = Ci,
      D = Di
    ),
    trace = F
  )

# simplified SA HT method ------------
fit_sa_simp_bad <-
  nlsLM(
    sa_ss ~ func(temp_f,Ai_sa, Bi_sa, C, D),
    data = cvn_data_bad ,
    start = list(
      C = Ci,
      D = Di
    ),
    trace = F
  )

fit_sa_lm_bad <- lm(temp_f ~ k1, data = cvn_data_bad) 


SA_coefs_gen_bad <- tidy(fit_sa_bad,conf.int = FALSE) %>% 
  mutate(model="General Solution")

SA_coefs_simp_bad <- tidy(fit_sa_simp_bad,conf.int = FALSE) %>% 
  mutate(model="Simplified HT")

sa_lm_coefs_bad <- tidy(fit_sa_lm_bad, conf.int = FALSE) %>%
  mutate(term = ifelse(term == "k1", "C", "D"))

sa_coefs_all_bad <-  sa_lm_coefs_bad %>%
  bind_rows(tibble(term = c("A", "B"),
                   estimate = c(51, 49))) %>%
  mutate(model = "Simplified OLS") %>%
  bind_rows(SA_coefs_simp_bad) %>% 
  bind_rows(tibble(term = c("A", "B"),
                   estimate = c(51, 49), model = "Simplified HT")) %>%
  mutate(term = case_when(term == "(Intercept)" ~ "D",
                          term == "k1" ~ "C",
                          TRUE ~ term)) %>%
  bind_rows(SA_coefs_gen_bad) %>%
  mutate(across(.cols = everything(), ~ ifelse(is.na(.), 0, .))) %>%
  arrange(term, model)

  
cvn_data_bad %>%
  ggplot(aes(temp_f, sa_ss)) +
  geom_point() +
  stat_function(
    fun = func,
    args = list(
      sa_coefs_all_bad$estimate[1],
      sa_coefs_all_bad$estimate[4],
      sa_coefs_all_bad$estimate[7],
      sa_coefs_all_bad$estimate[10]
    ),
    aes(col = "General"),
    lwd = 1,
    n = 301
  ) +
  stat_function(
    fun = func,
    args = list(
      sa_coefs_all_bad$estimate[2],
      sa_coefs_all_bad$estimate[5],
      sa_coefs_all_bad$estimate[8],
      sa_coefs_all_bad$estimate[11]
    ),
    aes(col = "Simplified HT"),
    lwd = 1,
    n = 301
  ) +
  stat_function(
    fun = func,
    args = list(
      sa_coefs_all_bad$estimate[3],
      sa_coefs_all_bad$estimate[6],
      sa_coefs_all_bad$estimate[9],
      sa_coefs_all_bad$estimate[12]
    ),
    aes(col = "Simplified OLS"),
    lwd = 1,
    n = 301
  ) +
  geom_hline(
    yintercept = 85,
    col = 'grey50',
    lty = 2,
    lwd = 0.65
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(x = "Temperature (\u00B0F)",
       y = "Shear Area (%)") +
  annotate(
    "text",
    x = 20,
    y = 85,
    label = "SATT",
    vjust = -0.5
  ) +
  scale_color_manual(
    values = cols,
    breaks = c("General", "Simplified HT", "Simplified OLS"),
    name = "Model"
  )


```

## Absorbed Energy with High Scatter Data  
In this section the same data that was used in @fig-badsa-data will be used for the absorbed energy and the three solution methods compared. Comparing the model solutions in @fig-ae-baddata it can be seen that the errors propagated from the shear area solution made a significant difference for the simplified solutions.  There is about a 25% difference in upper shelf energy between the general solution and the simplified OLS solution with the simplified HT solution splitting the difference between them. Even though the linearized OLS solution will give nearly the same solution as the simplified HT solution with consistent data, when there are large outliers and inconsistent data the OLS solution is more influenced by these outlier observations compared to the HT solutions.

```{r}
#| label: fig-ae-baddata
#| fig-cap: "Absorbed Energy Solution with High Scatter Data"
#| dpi: 300

# intital values for AE solution
ae_init_bad <- cvn_data_bad %>% 
  summarise(Ai_ae = max(cvn)/2,
            Bi_ae = (max(cvn) - min(cvn))/2)


fit_cvn_bad <-
  nlsLM(
    cvn ~ func(temp_f, A, B, C, D),
    data = cvn_data_bad ,
    start = list(
      A = ae_init_bad$Ai_ae,
      B = ae_init_bad$Bi_ae,
      C = Ci,
      D = Di
    ),
    trace = F
  )


sa_simp_sols_bad <- tibble(C = SA_coefs_simp_bad$estimate[1], D =SA_coefs_simp_bad$estimate[2])

# AE solutions ------------------------------------------------------------


fit_cvn_simp_bad <-
  nlsLM(
    cvn ~ func(temp_f, A, B, sa_simp_sols_bad$C, sa_simp_sols_bad$D),
    data = cvn_data_bad ,
    start = list(
      A = ae_init_bad$Ai_ae,
      B = ae_init_bad$Bi_ae
    ),
    trace = F
  )

AE_gen_coefs_bad <- tidy(fit_cvn_bad, conf.int = FALSE) %>% 
  mutate(model = "General Solution")

AE_coefs_simp_bad <- tidy(fit_cvn_simp_bad, conf.int = FALSE) %>% 
  bind_rows(SA_coefs_simp_bad) %>% 
  mutate(model = "Simplified HT")

cvn_data_bad <- cvn_data_bad %>% 
  mutate(k2 = tanh((temp_f - sa_lm_coefs_bad$estimate[1]) / sa_lm_coefs_bad$estimate[2]))

ae_lm_mod_bad <- lm(cvn ~ k2, data = cvn_data_bad)

ae_lm_coefs_bad <- tidy(ae_lm_mod_bad, conf.int = FALSE) %>% 
  mutate(term = ifelse(term=="k2", "B", "A")) %>% 
  bind_rows(sa_lm_coefs_bad) %>% 
  arrange(term) %>% 
  mutate(model = "Simplified OLS")

ae_coefs_all_bad <- AE_gen_coefs_bad %>% 
  bind_rows(ae_lm_coefs_bad,AE_coefs_simp_bad) %>% 
  arrange(term, model)


# plot AE solutions -------------------------------------------------------

cvn_data_bad %>%
  ggplot(aes(temp_f, cvn)) +
  geom_point() +
  stat_function(
    fun = func,
    args = list(
      ae_coefs_all_bad$estimate[1],
      ae_coefs_all_bad$estimate[4],
      ae_coefs_all_bad$estimate[7],
      ae_coefs_all_bad$estimate[10]
    ),
    aes(col = "General"),
    lwd = 1,
    n = 301
  ) +
  stat_function(
    fun = func,
    args = list(
      ae_coefs_all_bad$estimate[2],
      ae_coefs_all_bad$estimate[5],
      ae_coefs_all_bad$estimate[8],
      ae_coefs_all_bad$estimate[11]
    ),
    aes(col = "Simplified HT"),
    lwd = 1,
    n = 301
  ) +
  stat_function(
    fun = func,
    args = list(
      ae_coefs_all_bad$estimate[3],
      ae_coefs_all_bad$estimate[6],
      ae_coefs_all_bad$estimate[9],
      ae_coefs_all_bad$estimate[12]
    ),
    aes(col = "Simplified OLS"),
    lwd = 1,
    n = 301
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  labs(x = "Temperature (\u00B0F)",
       y = "Absorbed Energy") +
  scale_color_manual(
    values = cols,
    breaks = c("General", "Simplified HT", "Simplified OLS"),
    name = "Solution"
  )

```

